mean
variance
skewness
Kurtosis

mean,median,mode
variance,SD


mean -> average
median -> midpoint

10,20,30,40,50 -> mean=150/5=30
               -> median=30

15,000 -2,00,000

		number of occurence of an event
probability = -------------------------------------
		total number of occurence 

why ML?

sample = { (1,1),(1,2)........ (6,6)}

A -> B => dep

A B => indep

A  c - B => conditionally dep

			    P(A and B)
conditional probability =  -------------- = P(A/B)  = 0.35/0.7 = 0.5
			      P(B)

			P(A) P(B/A)
Navie Baye's, P(A/B) = --------------
			   P(B)

70% like chocolate
35% like strawberry and chocolate 

those who like chocolate also like strawberry -> P(strawberry/chocolate)

SMHT - yes?/no?

		P(yes) P(SMHT/yes)
P(yes/SMHT) = -----------------------
		P(S).P(M).P(H).P(T)

		P(no) P(SMHT/no)
P(no/SMHT) = ------------------------
		P(S).P(M).P(H).P(T)

Entropy -> measure of randomness (thermodynamics)

Message/Information --> sun rises in the east , entropy is the measure of randomness 

Entropy -> measure of information content

Entropy (H) = - P log_2 (P)

	     log_10 (x)		log_10 (x)
log_2 (x) = --------------  = --------------
	     log_10 (2)		  0.3010

p1 = 0.5

h = - 0.5 log2 (0.5)

     log10 (0.5)       -0.3010
h = ------------  =  ------------ = -1
      0.3010	        0.3010

H1 = -0.5 * -1 = 0.5

algorithm using entropy is decision tree

Information = High  --> Uncertain or most unlikely event

Information = Low   --> Certain or most likely event

Information = twice --> independent event or mutually exclusive 

Stochostic Process 

part of statistics which deals with random process/random events

			random variable (x)
		     __________|____________
		 Continuous		  Discrete

Probability Distribution function (X)

poisson,exponential,gaussian,binomial,uniform 

60% of people who purchase sports cars are men.  If 10 sports car owners are randomly selected, find the probability that exactly 7 are men.

probability mass function (PMF) -> discrete -> summation
probability density function(PDF) -> continuous -> integeral
marginal probability => 0-60 -> 5seconds,0-20 seconds?
joint probability (X and Y) 

HYPOTHESIS

---------------------------------------------------------------------------------------------------------------------------

						Linear Algebra

vectors and matrix
			-5 _______________________ 0

scalar  -> magnitude = 5

vector  -> magnitude and direction => (-5,0)

matrix  -> 2D data 

Tensor  -> 3D data


Matrix => by solving Ax = b,we can get to know the given matrix has a solution?

A -> input matrix
b -> constant value
x -> variable

how to find a solution?
1. Rank method
2. Carmer's rule

Determinant:- square matrix (m x n, m=n) i.e., m=> row, n=> column

     2  4  7       x  x  x
A =  3  5  2   =>  0  x  x
     1  0  4       0  0  x

|A| = 2 (20 - 0) -4 (12 - 2) + 7 (0 - 5)
    = 2 (20) -4 (10) +7 (-5)
    = 40-40-35
    = -35


Carmer's rule:-

1. |A|!=0 ,|Ax|=|Ay|=|Az|!=0  ---> unique solution

2. |A|=0 ,|Ax|=|Ay|=|Az|!=0   ---> NO solution

3. |A|=0 ,|Ax|=|Ay|=|Az|=0    ---> many/infinite solution


x+y+2z = 4, 2x+2y+4z = 8, 3x+3y+6z = 10

   1  1  2   x   4
=> 2  2  4 * y = 8
   3  3  6   z   10

|A|  = 0

|Ax| = 0

|Ay| = 0

|Az| = 0

infinite solutions

z = k

x-y   = 4 - 2k
2x+2y = 8 - 4k
3x+3y = 10 - 6k

 2x - 2y =  8 - 4k
 2x + 2y =  8 - 4k
-   -      -  + 

x=0
y=4 - 2k
z=k
(x,y,z) = (0,4-2k,k) k belongs to any real number

why matrix?

input data => text, number, image/video, audio

image/video => video (FPS) 

image => HD image 1080x1920 -> resolution
pixel -> dot per inch(dpi)

colors => 16M

primary colors = RGB
Secondary colors = CYM

white,black

(12,45,136)

text->conversion->vector

conversion method-> word embedding and word2vec

word embedding-> pre trained embedding -> GLoVe

word2vec -> count vectorizer -> based on frequency (n-gram/skip gram)

n=1 --> unigram, n=2 --> bigram

"My friend name is John and his pet's name is tommy."

processed text:-

"friend name John pet name tommy" ==> [(0,1),(1,2),(2,1),(3,1),(4,1)]

	unigram				bigram

0 (friend,(0,1))		friend name (0,1)
1 name (0,2)			name john (0,1)
2 john (0,1)			john pet  (0,1)
3 pet (0,1)			pet name  (0,1)
4 tommy (0,1)			name tommy (0,1)

Types of matrix

row matrix -> [1,2,3]

column -> | 2 |
	  | 4 |
	  | 5 |

square matrix -> m x n if m=n


A.At = 1

Decomposition

12 -> 3 x 4 ,6 x 2

{3,2}

1044

Square Matrix -> Eigen Decomposition

Non-Square Matrix -> SVD (singular value decomposition)

PCA - principle component analysis

Problems in AI:-

1. Poor conditioning ==> x -> y, x+1 -> y+1

2. Overfit and underfit

3.  

---------------------------------------------------------------------------------------------------------------------------
ML Algorithms:-

1. Linear regression -> regression
2. Logistic regression -> Classification
3. SVM -> Classification
4. K-means -> clustering
5. KNN  -> clustering
6. Hierarchy clustering -> clustering
7. LDA -> topic modeling
8. TF-IDF -> topic modeling
9. Eclat -> associative learning
10. Aprior -> associative learning
11. Random forest -> Ensemble method
12. AdaBoost -> Ensemble method
13. Decision tree -> Classification
14. Naive Baye's -> Classification
15. PCA -> Dimensionality reduction (coding is not yet completed)

Neural Network:-

1. perceptron
2. sigmoid
3. multi layer preceptron
4. Backpropagation
4.a gradient descent
4.b stochostic gradient descent
5. Vanila neural network

Steps:-
1. Load dataset
2. Split dataset (70:30)
3. Define an algorithm
4. Training (70%)
5. Validation - accuracy = 100%
6. Tuning
7. Testing (30%) >= 85%

Assume:-
1. Application
2. Algorithms
3. Datasets

---------------------------------------------------------------------------------------------------------------------------
1. PCA

number of person -> 8
p=8

each person has 5 images ==> M = 40

single image size => 3 x 3 -> 9 x 1

5 images size => 45 x 1

overall matrix => 45 x 8

mean image size => 45 x 1


optimum image processing size => 28 x  28, 64 x 64, 96 x 96, 

45 x 8 . 8 x 45 = 45 x 45

8 x 45 . 45 x 8 = 8 x 8

eigen vector(8 x 8)

eigen space

norm or norm distance

http://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/

















